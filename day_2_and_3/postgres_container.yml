services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always

docker run -it -e  POSTGRES_USER="root" -e POSTGRES_PASSWORD="root" -e  POSTGRES_DB="ny_taxi" -v  C:/Users/U/Desktop/data_engineering/day2/ny_taxi_postgres_data:/var/lib/postgresql/data -p  5432:5432  postgres:13


pip install pgcli
pgcli -h localhost -p 5432 -u root -d ny_taxi
\dt
\d tablename

#DAY 3 CONNECTING TO PGADMIN USING DOCKER CONTAINER


docker run -it -e PGADMIN_DEFAULT_EMAIL="admin@admin.com" -e PGADMIN_DEFAULT_PASSWORD="root" -p 8080:80 dpage/pgadmin4


#create network 
docker network create pg-network

#pg database
docker run -it -e  POSTGRES_USER="root" -e POSTGRES_PASSWORD="root" -e  POSTGRES_DB="ny_taxi" -v  C:/Users/U/Desktop/data_engineering/day2/ny_taxi_postgres_data:/var/lib/postgresql/data -p  5432:5432  --network=pg-network --name=pg-database postgres:13 

#pg admin
docker run -it -e PGADMIN_DEFAULT_EMAIL="admin@admin.com" -e PGADMIN_DEFAULT_PASSWORD="root" -p 8080:80 --network=pg-network --name=pg-admin dpage/pgadmin4

#to remove running container 
docker rm container name running



#DOCKERIZING THE INGESTION PART OF DAY1

#covert jupyter notebook to python script file with py extension
jupyter nbconvert --to=script .\append_data_to_postgres_automated.ipynb



python append_data_to_postgres_automated.py --user=root --password=root --host=localhost --port=5432 --database=ny_taxi --table=taxi_data --file_path=day2\new_york.csv

docker build -t ingest_postgres:v001 .

#docker compose is used to run multiple containers in one container file name always be docker-compose.yaml or yml it is written in yaml format

docker-compose up #for up the compose
docker-compose down #for down the compose
docker-ccompose up -d #for up the compose and run in background